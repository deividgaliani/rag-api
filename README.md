# RAG API

Este projeto √© uma API REST para **RAG (Retrieval-Augmented Generation)** desenvolvida com **Spring Boot**, utilizando **LangChain4j**, **Ollama** e **PostgreSQL (pgvector)**. A aplica√ß√£o permite a ingest√£o de documentos PDF e a realiza√ß√£o de chats baseados no conte√∫do desses documentos.

## üöÄ Tecnologias Utilizadas

- **Java 21**
- **Spring Boot 3.4.0**
- **LangChain4j 0.35.0**
- **Ollama** (LLM e Embeddings)
- **PostgreSQL + pgvector** (Vector Database)
- **Docker & Docker Compose**

## üìã Pr√©-requisitos

Certifique-se de ter instalado em sua m√°quina:

- [Java JDK 21](https://adoptium.net/)
- [Maven](https://maven.apache.org/)
- [Docker](https://www.docker.com/) e Docker Compose
- [Ollama](https://ollama.com/) (executando localmente ou remotamente)

## üõ†Ô∏è Configura√ß√£o e Execu√ß√£o

### 1. Carregar Vari√°veis de Ambiente

Crie um arquivo `.env` na raiz do projeto (ou utilize os valores padr√£o do `application.yml`) com as seguintes configura√ß√µes:

```properties
DB_URL=jdbc:postgresql://localhost:5432/vector_store
DB_USER=user
DB_PASSWORD=password
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL_NAME=nomic-embed-text
OLLAMA_CHAT_MODEL=llama3.2
```

### 2. Iniciar o Banco de Dados

Utilize o Docker Compose para subir o container do PostgreSQL com a extens√£o pgvector j√° configurada:

```bash
docker-compose up -d
```

### 3. Executar a Aplica√ß√£o

Voc√™ pode executar a aplica√ß√£o via Maven:

```bash
./mvnw spring-boot:run
```

A API estar√° dispon√≠vel em `http://localhost:8080`.

## üìö Documenta√ß√£o da API (Swagger)

A documenta√ß√£o interativa da API pode ser acessada atrav√©s do Swagger UI:

- **URL:** `http://localhost:8080/swagger-ui.html`

## üîå Endpoints Principais

### Ingest√£o de Documentos

1. **Ingerir Diret√≥rio Local:**
   - **POST** `/api/ingest`
   - **Query Param:** `path` (Padr√£o: `docs`) - Caminho absoluto para a pasta contendo os PDFs.

2. **Upload de PDF √önico:**
   - **POST** `/api/ingest/pdf`
   - **Body:** `multipart/form-data` com o campo `file` contendo o arquivo PDF.

### Chat

- **POST** `/api/chat`
- **Body:**
  ```json
  {
    "question": "O que diz o documento sobre a arquitetura?"
  }
  ```

## üê≥ Docker (Opcional)

Se desejar rodar a aplica√ß√£o via Docker, certifique-se de criar a imagem ou utilizar um Dockerfile apropriado (n√£o inclu√≠do por padr√£o na raiz, mas configur√°vel).

## üîÑ Fluxo Completo de Recupera√ß√£o (RAG)

1.  **Entrada (`RagController`)**: O usu√°rio envia um POST com a pergunta.
2.  **Busca (`ContentRetriever`)**:
    *   A pergunta vira um vetor (usando `nomic-embed-text`).
    *   O sistema busca no PostgreSQL (`vector_store`) os 3 segmentos mais similares (score > 0.7).
3.  **Prompt**: O LangChain4j monta um prompt com os segmentos recuperados + a pergunta do usu√°rio.
4.  **Gera√ß√£o (`ChatLanguageModel`)**: A LLM (`llama3.2`) gera a resposta usando o contexto.

## ‚öôÔ∏è Detalhamento das Configura√ß√µes

### 1. `application.yml` (Estrutura)
Define os par√¢metros de ingest√£o e conex√£o:
*   `rag.ingestion.document-splitter`:
    *   `max-segment-size`: Tamanho do peda√ßo de texto (ex: 2000 chars).
    *   `max-overlap-size`: Repeti√ß√£o para manter contexto (ex: 400 chars).

### 2. `.env` (Ambiente)
Controla vari√°veis din√¢micas sem mexer no c√≥digo:
*   `OLLAMA_MODEL_NAME`: Modelo de Embedding (ex: `nomic-embed-text`).
*   `OLLAMA_CHAT_MODEL`: Modelo de Chat (ex: `llama3.2`, `mistral`).

### 3. `RagConfig.java` (Comportamento)
Define regras de neg√≥cio do RAG:
*   `maxResults(3)`: Quantidade de trechos de documentos a enviar para a IA.
*   `minScore(0.7)`: Filtragem de relev√¢ncia (0.0 a 1.0).

## üìù Notas

- O modelo de embedding configurado deve estar dispon√≠vel no seu servidor Ollama (ex: `ollama pull nomic-embed-text`).
- O modelo de chat tamb√©m deve estar baixado (ex: `ollama pull llama3.2`).
